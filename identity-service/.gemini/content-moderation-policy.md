# Content Moderation Policy - Updated

## üìã T·ªïng quan

H·ªá th·ªëng ki·ªÉm duy·ªát n·ªôi dung blog s·ª≠ d·ª•ng **2 l·ªõp AI**:
1. **Google Vision API SafeSearch** - Ki·ªÉm tra h√¨nh ·∫£nh
2. **Gemini AI** - Ph√¢n t√≠ch t·ªïng h·ª£p (text + image reports)

## üéØ Image Safety Policy (Google Vision API)

### Likelihood Levels

| Level | √ù nghƒ©a | K·∫øt qu·∫£ |
|-------|---------|---------|
| **VERY_UNLIKELY** | R·∫•t kh√¥ng c√≥ kh·∫£ nƒÉng vi ph·∫°m | ‚úÖ **PASS** |
| **UNLIKELY** | Kh√¥ng c√≥ kh·∫£ nƒÉng vi ph·∫°m | ‚úÖ **PASS** |
| **POSSIBLE** | C√≥ th·ªÉ vi ph·∫°m | ‚ùå **UNSAFE** ‚Üí Staff review |
| **LIKELY** | C√≥ kh·∫£ nƒÉng vi ph·∫°m | ‚ùå **UNSAFE** ‚Üí Staff review |
| **VERY_LIKELY** | R·∫•t c√≥ kh·∫£ nƒÉng vi ph·∫°m | ‚ùå **UNSAFE** ‚Üí Staff review |

### Categories Checked

1. **Adult**: N·ªôi dung ng∆∞·ªùi l·ªõn, pornography
2. **Violence**: B·∫°o l·ª±c, gore, terrorism
3. **Racy**: N·ªôi dung g·ª£i c·∫£m, nh·∫°y c·∫£m
4. **Medical**: N·ªôi dung y t·∫ø nh·∫°y c·∫£m

### Decision Logic

```java
// ·∫¢nh ƒë∆∞·ª£c coi l√† SAFE khi T·∫§T C·∫¢ categories ƒë·ªÅu VERY_UNLIKELY ho·∫∑c UNLIKELY
boolean isSafe = !isLikely(adult) &&      
                 !isLikely(violence) &&   
                 !isLikely(racy) &&       
                 !isLikely(medical);

// isLikely() returns true n·∫øu level >= POSSIBLE
private boolean isLikely(Likelihood likelihood) {
    return likelihood == Likelihood.POSSIBLE || 
           likelihood == Likelihood.LIKELY || 
           likelihood == Likelihood.VERY_LIKELY;
}
```

### Examples

#### ‚úÖ SAFE Image
```
Cover Image: SAFE
- Adult: VERY_UNLIKELY
- Violence: VERY_UNLIKELY
- Racy: UNLIKELY
- Medical: VERY_UNLIKELY
‚Üí Result: PASS
```

#### ‚ùå UNSAFE Image (Requires Staff Review)
```
Cover Image: WARNING DETECTED [Adult: POSSIBLE, Violence: UNLIKELY, Racy: LIKELY, Medical: UNLIKELY]
‚Üí Result: UNSAFE (c√≥ POSSIBLE v√† LIKELY)
‚Üí Action: Blog status = AI_REJECTED, c·∫ßn staff duy·ªát l·∫°i
```

## ü§ñ Gemini AI Moderation Policy

### Image Analysis Rules

Gemini AI nh·∫≠n ƒë∆∞·ª£c b√°o c√°o t·ª´ Vision API v√† √°p d·ª•ng **STRICT RULES**:

1. ‚úÖ N·∫øu image report = **"SAFE"** ‚Üí Image passed
2. ‚ùå N·∫øu image report = **"WARNING DETECTED"** v·ªõi b·∫•t k·ª≥ category n√†o ·ªü m·ª©c **POSSIBLE/LIKELY/VERY_LIKELY**:
   - **MUST** flag as violation
   - Set `isSafe = false`
   - Add to `violations` array
   - Deduct points from `safetyScore`

3. **Even ONE category at POSSIBLE or higher = VIOLATION**

### Text Content Violations

Gemini AI c≈©ng ki·ªÉm tra text cho c√°c vi ph·∫°m:

1. Profanity, offensive language, vulgar words
2. Adult content, pornography, sexual content (18+)
3. Illegal drugs, controlled substances
4. Violence, gore, terrorism, threats
5. Fraud, scams, phishing attempts
6. Spam, excessive advertising
7. Hate speech, discrimination, harassment
8. Dangerous misinformation (health, safety)
9. Personal attacks, doxxing
10. Copyright infringement claims

### Safety Score Calculation

```
Base Score: 100 (completely safe)

Deductions:
- POSSIBLE: -20 to -30 points per violation
- LIKELY: -40 to -50 points per violation
- VERY_LIKELY: -60 to -80 points per violation

Final Score Range: 0-100
- 80-100: Safe
- 50-79: Moderate risk
- 0-49: High risk
```

### Analysis Approach

```
Step 1: Check ALL image reports
  ‚Üì
Step 2: If ANY image has POSSIBLE/LIKELY/VERY_LIKELY
  ‚Üí Mark as unsafe immediately
  ‚Üì
Step 3: Analyze text content for violations
  ‚Üì
Step 4: Combine image + text results
  ‚Üì
Step 5: Generate final decision
```

## üìä Workflow

### Blog Creation Flow

```
1. User creates blog with images
   ‚Üì
2. Blog status = PENDING_REVIEW
   ‚Üì
3. Wait 15 minutes (scheduled task)
   ‚Üì
4. ContentModerationService.moderatePendingBlogs()
   ‚Üì
5. For each image:
   - Call Vision API SafeSearch
   - Get likelihood levels
   - Determine SAFE or WARNING
   ‚Üì
6. Build content with image reports
   ‚Üì
7. Send to Gemini AI
   ‚Üì
8. Gemini analyzes:
   - Image reports (STRICT policy)
   - Text content
   ‚Üì
9. Decision:
   - isSafe = true ‚Üí Blog status = PUBLISHED
   - isSafe = false ‚Üí Blog status = AI_REJECTED
   ‚Üì
10. If AI_REJECTED:
    - Save to BlogModerationHistory
    - Notify staff for manual review
```

## üéØ Decision Matrix

| Image Status | Text Status | Final Decision | Blog Status |
|--------------|-------------|----------------|-------------|
| All VERY_UNLIKELY/UNLIKELY | Clean | ‚úÖ SAFE | PUBLISHED |
| All VERY_UNLIKELY/UNLIKELY | Has violations | ‚ùå UNSAFE | AI_REJECTED |
| Has POSSIBLE/LIKELY/VERY_LIKELY | Clean | ‚ùå UNSAFE | AI_REJECTED |
| Has POSSIBLE/LIKELY/VERY_LIKELY | Has violations | ‚ùå UNSAFE | AI_REJECTED |

## üìù Example Scenarios

### Scenario 1: Clean Blog
```
Images: All VERY_UNLIKELY/UNLIKELY
Text: No violations
‚Üí isSafe = true
‚Üí safetyScore = 95-100
‚Üí Blog status = PUBLISHED
```

### Scenario 2: Suspicious Image
```
Cover Image: Adult = POSSIBLE
Text: Clean
‚Üí isSafe = false
‚Üí safetyScore = 70-80 (deduct 20-30)
‚Üí violations = ["Potentially inappropriate image content"]
‚Üí Blog status = AI_REJECTED
‚Üí Action: Staff manual review required
```

### Scenario 3: Violent Content
```
Section 1 Image: Violence = LIKELY
Text: Contains violent language
‚Üí isSafe = false
‚Üí safetyScore = 40-60 (deduct 40-50 for image + text)
‚Üí violations = ["Violent imagery", "Violent language"]
‚Üí Blog status = AI_REJECTED
‚Üí Action: Staff manual review required
```

### Scenario 4: Adult Content
```
Cover Image: Adult = VERY_LIKELY, Racy = LIKELY
Text: Contains adult references
‚Üí isSafe = false
‚Üí safetyScore = 0-20 (heavy deductions)
‚Üí violations = ["Adult content in images", "Adult content in text"]
‚Üí Blog status = AI_REJECTED
‚Üí Action: Likely permanent rejection
```

## üîß Configuration

### Vision API Settings
```java
// ContentModerationService.java
private boolean isLikely(Likelihood likelihood) {
    return likelihood == Likelihood.POSSIBLE || 
           likelihood == Likelihood.LIKELY || 
           likelihood == Likelihood.VERY_LIKELY;
}
```

### Gemini AI Prompt
- Structured with clear sections
- Explicit policy definitions
- Step-by-step analysis approach
- Strict enforcement rules
- Detailed scoring guidelines

## üìà Monitoring

### Metrics to Track
1. Total blogs moderated
2. Auto-approved rate (isSafe = true)
3. Auto-rejected rate (isSafe = false)
4. Staff review queue size
5. False positive rate (after staff review)
6. False negative rate (user reports)

### Logging
```java
log.info("Image safety check: {} - isSafe={}", imageUrl, isSafe);
log.info("Blog {} moderation: isSafe={}, score={}", blogId, isSafe, score);
```

## üö® Edge Cases

### Case 1: Vision API Failure
```
If Vision API fails for an image:
‚Üí Return "FAILED to analyze"
‚Üí Gemini AI treats as potential risk
‚Üí Blog may be flagged for staff review
```

### Case 2: Gemini AI Parsing Error
```
If Gemini response cannot be parsed:
‚Üí Default: isSafe = false, score = 50
‚Üí reason = "Unable to parse AI response. Manual review required."
‚Üí Blog status = AI_REJECTED
```

### Case 3: No Images
```
If blog has no images:
‚Üí Skip Vision API calls
‚Üí Only text moderation by Gemini
‚Üí Decision based purely on text content
```

## ‚úÖ Best Practices

1. **Conservative Approach**: When in doubt, flag for review
2. **Transparency**: Log all decisions with reasons
3. **Audit Trail**: Save moderation history
4. **Staff Override**: Allow manual approval/rejection
5. **Continuous Improvement**: Monitor false positives/negatives
6. **User Communication**: Notify users of rejections with clear reasons

## üîÑ Future Enhancements

1. Machine learning feedback loop from staff reviews
2. User appeal system
3. Category-specific thresholds
4. Context-aware moderation (e.g., medical blogs)
5. Multi-language support
6. Real-time moderation (not just scheduled)
